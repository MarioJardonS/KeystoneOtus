---
title: "Estimación de especies clave desde redes de correlación según un análisis de área bajo la curva con algunas medidas de centralidad"
output: html_document
date: '2022-10-21'
---

#Introducción

Los códigos de R aquí presentados se plantean como una posibilidad para estimar las especies clave de cierto tipo de microbiomas (o ecosistemas en general). Se parte de un conjunto de muestras de microbiomas y una red de correlación (o de coocurrencia), ya construida a partir de dichas muestras. Las muestras de microbiomas, siendo en general del mismo tipo, deben estar agrupadas según consideraciones ecológicas. Por su parte, cada nodo de la red de correlación representa un OTU, y el peso de cada arista representa la correlación (según cierta medida) entre los respectivos OTUs. Se orderán decrecientemente los OTUs según diversas medidas de centralidad dentro de la red y el segmento inicial que maximice la diferencia entre los grupos de muestras y maximice el parecido dentro de los grupos serán nuestros candidatos a especie clave. Las idea general proviene de . En este caso se ejemplificará el funcionamiento de este código con unas muestras de rizósfera de tomate y la respectiva red de correlación de Spearman. Para los análisis de centralidad se usará el paquete igraph aplicada a la red binaria que solo contemple las correlaciones positivas (y las iguale a 1). Los datos se pueden encontrar en el repositorio redes_correlacion_coocurrencia. 


#Carga de paquetes y datos

##Paquetes

Aquí se presentan los paquetes que serán necesarios para nuestro análisis. "vegan" se usará para calcular disimilaridades de Bray-Curtis entre las muestras. "igraph" será utilizada para calcular la componente conexa principal y las medidas de centralidad. "apcluster" será usada para descartar muestras "ruido". "plyr" se usará para simplificar la aplicación de funciones a listas. La celda 1 se correrá solo con los paquetes no instalados. La celda 2 consiste en llamar estos paquetes.

```{r}
#1
#install.packages("vegan")
#install.packages("igraph")
#install.packages("apcluster")
#install.packages("plyr")
```

```{r}
#2
library(vegan)
library(igraph)
library(apcluster)
library(plyr)
```

#Carga y análisis de las muestras 

Como primer paso se carga la tabla de muestras, en este caso de rizósfera de tomate, como un dataframe de R. Los "nombres", presentes en la primera columna, se refieren al ID de NCBI a nivel especie y se usa como nombres de las filas del dataframe. Las demás columnas se refieren a las instancias (reads asignados?) de los otus por muestra. 

```{r}
#getwd()
#setwd("~/proyecto_redes")
data <- read.table("./redes_correlacion_coocurrencia/data/table.from_tomate.txt", row.names = 1, header = FALSE , sep= "" )
head(data)
```

##Agrupación de muestras y descarte de "outliers"

A continuación se agrupan las muestras según los metadatos presentes en el archivo "data/fastp_metadat.csv". En este caso, cada uno de los vectores se corresponde a una "etapa fenológica" del cultivo del tomate. Los elementos de las vectores son los nombres de las columnas del dataframe "data" correspondientes a cada grupo.

```{r}
produccion <- c("V2", "V3" ,"V4","V5")
llenado_de_fruto <- c("V6", "V7", "V8")
#plantacion <- c()
por_transplantar <- c("V9")
desarrollo <- c("V10", "V11", "V12", "V13", "V14", "V15", "V16", "V17","V18","V19","V20","V21","V22","V23")
```

El análisis aquí planteado busca maximizar el parecido entre las muestras de un mismo grupo. Por esa razón se excluirán las muestras que al estar muy separadas de las demás, y por ende de su propio grupo, probablemente generen ruido. Las muestras que se considerarán outliers son las que queden en un clúster unitario tras una propagación de afinidad. 

```{r}
bc_dist <- vegdist(t(data), method = "bray")
s <- 1 - bc_dist
s <- as.matrix(s)
clustering <- apcluster(s)

clusters <- clustering@clusters
filtro_0 <- lapply(clusters, length)
clusters_no_outliers <- clusters[which(filtro_0 > 1)]
no_outliers <- unlist(clusters_no_outliers)
no_outliers <- names(no_outliers)

```

En la siguiente celda finalmente se reducen las muestras (en la variable "data") y los grupos fenológicos a las no "outliers".

```{r}
data <- data[,no_outliers]

grupos <- list()
grupos[[1]] <- intersect(produccion,no_outliers)
grupos[[2]] <- intersect(llenado_de_fruto,no_outliers)
grupos[[3]] <- intersect(por_transplantar,no_outliers)
grupos[[4]] <- intersect(desarrollo,no_outliers)

#Grupos necesarios para el análisis auc
len_list <- llply(grupos , length)
len_list <- which(len_list > 1)
grupos <- grupos[len_list]
```

##Filtración de OTUs según su aparición en las muestras

Se filtrarán los OTUs que aparezcan en una sola muestra, dado que estos OTUs tendrán un grado artificialmente alto en la red de correlación sin que esto refleje una "centralidad ecológica". Dado que los nodos en el archivo "data/networks/tomate_species_raw_network.csv" están ya numerados desde 0, una columna de nodos se agrega a data para no perder la biyección entre otus y su nodo en la red. 

Filtraciones de este tipo se pueden generalizar a filtrar otus que aparezcan, por ejemplo, al menos 10 veces en solamente menos de 10% de las muestras. En futuros análisis estos umbrales podrían ajustarse. 

```{r}
data$nodos <- 0:(dim(data)[1]-1)

filt <- c()
for (i in 1:dim(data)[1]) {
  
    v_i <- as.vector(data[i,1:(dim(data)[2]-1)])
    #el siguiente 1 es filtro
      if (length(v_i [ v_i > 0 ]) > 1 ) {
        filt <- c(filt, i)
        }
     }
 
data <- data[filt,]
```

#Carga y análisis de la red

El siguiente paso es cargar la red de coocurrencia correspondiente a nuestras muestras. En este caso se trata de una red de correlación de Spearman, que está en el archivo "data/networks/tomate_species_raw_network.csv". Se trata de una red ponderada no dirigida cuyos pesos se encuentran entre -1 y 1. Cada una de las filas de este archivo se refiere a una arista de la red: las primeras dos entradas son los nodos correspondientes y la tercera es su peso. Este archivo se cargará como dataframe para el análisis previo a su conversión a red binaria no dirigida como objeto igraph. 


```{r}
red <- read.csv("./redes_correlacion_coocurrencia/data/networks/tomate_species_raw_network.csv")
red = red[,1:3]
```
##Conversión a red binaria y limpieza previa al análisis igraph

En la siguiente celda se filtran las aristas de la red con el siguiente criterio: los nodos deben estar en data$nodos, es decir, deben haber sobrevivido a la filtración de OTUs, y su correlación debe ser positiva. En la red final estas aristas serán consideradas de peso 1. 

```{r}
edges <- c()
for (i in 1:dim(red)[1]) {
  if (is.element(red[i,1], data$nodos) && is.element(red[i,2], data$nodos) && red[i,3] > 0  ){
    edges <- c(edges , i)
  }
}

red <- red[edges, 1:2]


```
En las siguientes dos celdas las etiquetas, hasta ahora numéricas, se transforman en los dataframes red y data a "strings" del tipo "v_#" para simplificar el trabajo con la librería igraph.

```{r}
red <- red + 1

for (i in 1:dim(red)[1]){
  for (j in 1:dim(red)[2]){
    red[i,j] <- paste("v_",as.character(red[i,j]))
  }
}
```

```{r}
data$nodos <- data$nodos + 1

for (i in 1:dim(data)[1]){
  data[i,"nodos"] <- paste("v_" , as.character(data[i,"nodos"]))
}
```

##Conversión de la red a objeto igraph y obtención de componente conexa principal


net_work <- graph_from_edgelist(as.matrix(red) , directed = FALSE )



##componente(s) conexa(s) principal(es)
compo_conexas <- components(net_work)
size_compo_conexas <- compo_conexas$csize
princ <- which(size_compo_conexas == max(size_compo_conexas))
pertenencia <- compo_conexas$membership
compo_princ <- which(pertenencia == princ )
compo_princ <- names(compo_princ)

##nuevos datos

filtro_componente <- c()
for (i in 1:dim(data)[1]){
  if(is.element(data[i,"nodos"],compo_princ)){
    filtro_componente <- c(filtro_componente, i)
  }
}


data <- data[filtro_componente,]
net_work <- induced_subgraph(net_work, compo_princ ,"auto")



######CALCULO DE MEDIDAS DE CENTRALIDAD############
degrees <- c()
for (i in 1:dim(data)[1]) {
  d_i <- degree(net_work, data[i,"nodos"])
  degrees <- c(degrees, d_i)
}
data$degrees <- degrees


## -----------------------------------------------------------------------------------------------------------------------
closeness_cent <- c()
for (i in 1:dim(data)[1]) {
  c_i <- closeness(net_work, data[i,"nodos"])
  closeness_cent <- c(closeness_cent, c_i)
}
data$closeness <- closeness_cent


## -----------------------------------------------------------------------------------------------------------------------
betweenness_cent <- c()
for (i in 1:dim(data)[1]) {
  b_i <- betweenness(net_work, data[i,"nodos"])
  betweenness_cent <- c(betweenness_cent, b_i)
}
data$betweenness <- betweenness_cent


data_deg <- data[order(data$degrees, decreasing = TRUE),]
data_close <- data[order(data$closeness , decreasing = TRUE),]
data_between <- data[order(data$betweenness, decreasing = TRUE),]

write.csv(data_deg,"./redes_correlacion_coocurrencia/results/table.from_tomate_bydegrees.csv", row.names = TRUE)
write.csv(data_close,"./redes_correlacion_coocurrencia/results/table.from_tomate_bycloseness.csv", row.names = TRUE)
write.csv(data_between,"./redes_correlacion_coocurrencia/results/table.from_tomate_bybetweenness.csv", row.names = TRUE)
## -----------------------------------------------------------------------------------------------------------------------

#Esta función calcula el área bajo la curva de una función representada como un dataframe, donde la columna 1 es el dominio, y sus imágenes están en la columna con nombre feature. 
auc <- function(df, centrality){
  sm_p <- 0
  for (i in 1:dim(df)[1]) {
  f_i = df[i, centrality]
  if (is.nan(f_i) == FALSE)
  {sm_p = sm_p + f_i}
}

return(sm_p)}


## -----------------------------------------------------------------------------------------------------------------------
#Con un dataframe que represente una función, y una cota superior (se espera que sea una fracción del área bajo la curva de dicha funcion). Nos devuelve la cantidad de "otus", como están ordenados según el dataframe, que alcanzan dicha cota.
auc_percent <- function(df, centrality, bound){
  #df es nuestro dataframe de muestras y medidas de centralidad, centrality un string con el nombre de nuestra centralidad, y bound un número positivo 
  
  i <- 1
  sum_par <- 0
  #c <- c() 
  while(i <= dim(df)[1] && sum_par < bound) {
    #c <- c(c,i)
    g_i = df[i, centrality]
    sum_par = sum_par + g_i
    i = i+1
  }
  return(i-1)
}


## -----------------------------------------------------------------------------------------------------------------------
library(plyr)
pseudo_F <- function(df , groups){
  #df es dataframe de muestras, groups, una lista de vectores de etiquetas por grupo , por grupos
  N <- dim(df)[2] #número de muestras
  a <- length(groups) #número de grupos
  
  df_groups <- list() #se crea la lista que incluirá los subdataframes por grupo
  
  for (i in 1:length(groups)){
    df_i <- df[,groups[[i]]]
    df_groups[[i]] <- df_i
  } 
  
  
  
  
  n_s <- llply( .data = df_groups , .fun = ncol )
  n_s <- unlist(n_s)
  n <- mean(n_s) #número promedio de muestras por grupo
  
  dist <- vegdist(t(df))
  dist <- as.vector(dist) #distancias bray_curtis entre todo par de muestras
  
 
  df_groups <- llply(.data = df_groups , .fun = t)
  in_dist <- llply(.data = df_groups , .fun = vegdist )
  in_dist <- llply(.data = in_dist , .fun = as.vector )
  in_dist <- unlist(in_dist) #distancias bray-curtis intra-grupo
  
  #calculo del estadistico pseudo-f
  
  ss_t <- sum(dist^2)/N 
  
  ss_w <- sum(in_dist^2)/n
  
  ss_a <- ss_t - ss_w

  f_stat <- (ss_a/(a-1))/(ss_w/(N-a))
  
  return(f_stat)
}



## -----------------------------------------------------------------------------------------------------------------------
#Se o



## -----------------------------------------------------------------------------------------------------------------------
area_deg <- auc(data_deg , "degrees")
auc5_percent_deg <- c()
for (x in 1:20){
  auc5_percent_deg = c(auc5_percent_deg , auc_percent(data_deg, "degrees" ,(area_deg/20)*x))
  
}


## -----------------------------------------------------------------------------------------------------------------------
area_close <- auc(data_close , "closeness")
auc5_percent_close <- c()
for (x in 1:20){
  auc5_percent_close = c(auc5_percent_close , auc_percent(data_close, "closeness" ,(area_close/20)*x))
  
}


## -----------------------------------------------------------------------------------------------------------------------
area_between <- auc(data_between , "betweenness")
auc5_percent_between <- c()
for (x in 1:20){
  auc5_percent_between = c(auc5_percent_between , auc_percent(data_between, "betweenness" ,(area/20)*x))
  
}


## -----------------------------------------------------------------------------------------------------------------------
f_stat_deg <- c()
var_deg <- c()
for (i in auc5_percent_deg){
  df_i <- data_deg[1:i ,1:18]
  f_i <- pseudo_F(df_i , grupos)
  f_stat_deg <- c(f_stat_deg , f_i) 
  
  var_i <- var(f_stat_deg)
  var_deg <- c(var_deg, var_i)
}

analisis_auc_deg <- data.frame(auc5_percent_deg, f_stat_deg , var_deg)
write.csv(analisis_auc_deg, "./redes_correlacion_coocurrencia/results/tomate_analisis_auc_bydegrees.csv")

## -----------------------------------------------------------------------------------------------------------------------
f_stat_close <- c()
var_close <- c()
for (i in auc5_percent_close){
  df_i <- data_close[1:i ,1:18]
  f_i <- pseudo_F(df_i , grupos)
  f_stat_close <- c(f_stat_close , f_i) 
  
  var_i <- var(f_stat_close)
  var_close <- c(var_close, var_i)
}


analisis_auc_close <- data.frame(auc5_percent_close, f_stat_close , var_close)
write.csv(analisis_auc_close, "./redes_correlacion_coocurrencia/results/tomate_analisis_auc_bycloseness.csv")


## -----------------------------------------------------------------------------------------------------------------------
f_stat_between <- c()
var_between <- c()
for (i in auc5_percent_between){
  df_i <- data_between[1:i ,1:18]
  f_i <- pseudo_F(df_i , grupos)
  f_stat_between <- c(f_stat_between , f_i) 
  
  var_i <- var(f_stat_between)
  var_between <- c(var_between ,var_i)
}

analisis_auc_between <- data.frame(auc5_percent_between, f_stat_between , var_between)
write.csv(analisis_auc_between, "./redes_correlacion_coocurrencia/results/tomate_analisis_auc_bybetweenness.csv")

